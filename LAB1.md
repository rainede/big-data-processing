LAB 1 - THE MAP/REDUCE PROGRAMMING MODEL
LAB 1: BASIC MAPREDUCE JOBS
Week 2 lab, MapReduce

October 5th 2017

 This lab sheet will guide you through the creation of your first MapReduce program.

 The objectives of this session are:

Understand the Map Reduce programming model
Familiarise with the Hadoop API
Setup a simple Hadoop MapReduce project
Run Hadoop jobs in standalone mode
INTRODUCTION
The instructions contained in this sheet as well as the attached configuration files assume that you are working in the ITL in Linux. If you wish to go through this lab sheet in another setup you will have to download and setup Hadoop in your machine, and update all the relevant paths to match the configuration of your computer. 

The most simple way to setup Hadoop in your machine is to run a preconfigured virtual machine, such as the Quickstart VM prepared by Cloudera. This VM uses the CDH distribution of Hadoop which is the same one used in the ITL

http://www.cloudera.com/content/cloudera/en/documentation/core/v5-2-x/topics/cloudera_quickstart_vm.html

Otherwise, you can also install and configure Hadoop in your machine, as long as it runs  Linux or OSX. You can find Hadoop installation instructions for your computer at:

http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html

This lab sheet uses Apache Ant for automating the build process of the MapReduce project.  For a complete reference on the ant building system you can check   http://ant.apache.org

You can use either an IDE (Eclipse, Netbeans), or your favorite code editor to create the Hadoop classes. However, the project must be built and packaged using the Ant script.

The following Youtube video provides a step by step guide on configuring Hadoop for Eclipse and Ant in the ITL environment:

[[(https://img.youtube.com/vi/9-dvoA4aWIw/0.jpg)](https://www.youtube.com/watch?v=9-dvoA4aWIw)


We are not going to use a Hadoop cluster for this session. Instead, we will just use Hadoop in standalone mode. While we won’t be able to exploit the resources of our computing cluster, this lab session will be focussed on familiarizing with the Hadoop programming model.

WORD COUNT IN HADOOP
In this session we are going to replicate the Word Count program covered in the lecture. However, we have to adapt it to the Hadoop project structure and API.

In Map/Reduce we will write three parts of the program: The Mapper, which implements the Map function, the Reducer, which implements the Reduce function, and the main class that configures the complete job. The input and output of both the Map and the Reduce function is a tuple of key and value. Hadoop defines several special classes to store the values from the tuple. In this first lab we will use the IntWritable object, which stores an int, and the Text object, which stores a String. The provided source code will show how to initialise and use these objects. 

Open the slides from week 1, and review the word count example. Make sure that you understand the basic flow: First we run the Mapper: This function splits the full text into all the individual words, and it generates one tuple for each word (with key being a String with the word, and value being 1). The Reducer receives as input the aggregated results from the Mapper execution: for each unique Key, it provides a list of values for that key that have been generated by the Mappers. With that information it must be able to count the number of occurrences for the word, and generate as result for that word another tuple, with key being a Text containing the String with the word, and value being a IntWritable with the total count stored as an integer.



1) First, create a folder for storing your project in your home user folder e.g. lab1.  

In a terminal type, mkdir -p bigdata/lab1

2) Download the ant build file (build.xml) available at QMPlus and copy it to the root of your project. This file is customized for the ITL You will need to update the hadoop.base.path, hadoop.version and hadoop.core.file property defined at the beginning of the file if you intend to work outside the ITL lab environment.

 The input for this job is going to be a small file (sherlock.txt) collecting several of the works of Arthur Conan Doyle (as packaged by Project Gutemberg). You can download this from QMPlus too.

 Create an input/ folder and copy the provided file to that folder.

 Now it is time to define the Java classes that will implement the complete MapReduce program

Create a new class in the src/ folder of your project named TokenizerMapper.java, and fill it with the following mapper code:

import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> { 
    private final IntWritable one = new IntWritable(1);
    private Text data = new Text();
    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        StringTokenizer itr = new StringTokenizer(value.toString(), "-- \t\n\r\f,.:;?![]'\"");
        while (itr.hasMoreTokens()) {
          data.set(itr.nextToken().toLowerCase());
          context.write(data, one);
        }
    }
}
 

Compare this code to the pseudocode that was displayed in the slides.

IntWritable and Text are the Hadoop equivalents to int and String Java types. They follow their own hierarchy of classes so that they can be moved over the network during the shuffle and partition steps.
Identify the code for splitting the text into separate words is slightly different, but the
The emit command is equivlent to  write in real MapReduce code.
Look at the signature of TokenizerMapper, and try to understand how it relates to pairs <k1,v1>, <k2,v2>
 

Now, create a new file in the src/ folder named IntSumReducer.java and copy the following skeleton of code. It is an incomplete implementation of the reducer.

import java.io.IOException;
import java.util.Iterator;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;


public class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context)

              throws IOException, InterruptedException {

        int sum = 0;

        for (IntWritable value : values) {

            //complete code here

        }

               result.set(sum);

        //complete code here

    }

}
  

Look at the provided structure and understand the differences between Hadoop classes and the pseudocode used in the slides. The code in the complete Mapper class provides a good starting point.

 After you have understood the reducer skeleton, complete the implementation by adding Java instructions to the lines of text marked ``complete code here’’.

 

Finally, we need to define a MapReduce program that orchestrates the two classes we have just defined. The following MapReduce Hadoop setup code does this task.  Name the file WordCount.java

 

import java.util.Arrays;
import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static void runJob(String[] input, String output) throws Exception {

        Configuration conf = new Configuration();

    Job job = new Job(conf);
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setReducerClass(IntSumReducer.class);
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IntWritable.class);
    Path outputPath = new Path(output);
    FileInputFormat.setInputPaths(job, StringUtils.join(input, ","));
    FileOutputFormat.setOutputPath(job, outputPath);
    outputPath.getFileSystem(conf).delete(outputPath,true);
    job.waitForCompletion(true);
  }

  public static void main(String[] args) throws Exception {
       runJob(Arrays.copyOfRange(args, 0, args.length-1), args[args.length-1]);
  }

}
 

The main method configures the Hadoop job through a JobConf object. Method calls explicitly specify the mapper and reducer, the path to the input files, the path where results will be written to, and the types of the output keys for the reducer.

The code of your mapreduce project is complete. In order to create a package ready to be executed by Hadoop go to the project root folder and invoke the ant command

ant clean dist
 If the code compiles correctly a compressed jar file named HadoopTest.jar will appear in a newly created dist/ folder

 Execute the job from the base folder of your project running the following line:

 hadoop-local  jar dist/WordCount.jar WordCount input out

If the job has executed correctly there should be two files in the out folder.

part-r-00000  _SUCCESS

You can quickly output the contents of the folder to see if it has worked by using either of the Unix commands cat or more.

The out folder will be automatically deleted if it exists as part of the Hadoop execution, so you'll lose the results unless you copy them to another folder.

ADDITIONAL QUESTIONS
Once you have completed the base Word Count exercise, try to answer correctly the following questions:

How many times does the word Sherlock appear in the file? The part-r-00000 file holds the answer. 
The Hadoop job outputs a set of statistics after the completion. That way, you can see what was the number of key-value pairs generated by the Mapper, the number of unique inputs to the Reducer, and the final result of the Reducer. Have a look at these values and answer the following two questions: How many words has the document in total? How many unique words does the document have?
Change the MapReduce job so that only the words that appear 3 or more times are actually stored in the output file
If you want to practice further, you can try these advanced questions. These questions are substantially more challenging than the previous ones, so only try them if you are up to date with the module labs and assignments.

Change the Hadoop job so that instead of counting words it counts bigrams (pairs of consecutive words). You can use the format "word1,word2", encoded in a Text(String) object for the keys to be sent between the Mapper and Reducer.
Change the Hadoop job so that you compute the total counts of word length for the dataset. That is, the number of words appearing in the text of length 1,2,3... Take into account that in this case you should change the intermediate key value types, so that they are [IntWritable, IntWritable] instead of [Text, IntWritable]. You will need to modify all three Java files to update the data types completely in Hadoop. What is the most common word length? You can discover that by inspecting the file manually, or running a Unix command over the results of the job: sort -n -k2 part-00000 | tail . However, that option would not be feasible with a large dataset. Try to define a second MapReduce project for computing this, using the output folder of the first job as the input for the second. 
